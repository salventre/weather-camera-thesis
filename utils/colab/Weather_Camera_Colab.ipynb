{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect to your GDrive"
      ],
      "metadata": {
        "id": "SldFFAxDgilX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oktz2O-YM_ta"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/Shareddrives/WeatherCamera')\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3wI4xw5n-5j"
      },
      "source": [
        "Install Packets and Check if we are using a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRtRglHaoAak"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8 #\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 #\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GpiGeoZfZI_"
      },
      "source": [
        "Check if we are using high-RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KTVLMGyMaNp"
      },
      "source": [
        "Extraction Project and Dataset from relative .zips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP47bAgxlbej"
      },
      "outputs": [],
      "source": [
        "# Extraction project\n",
        "PATH_TO_UNZIP_FILES = \"/home\"\n",
        "!unzip -o \"/content/drive/Shareddrives/WeatherCamera/Archives/weather-camera-thesis.zip\" -d $PATH_TO_UNZIP_FILES\n",
        "!mv \"/home/weather-camera-thesis-main\" \"/home/weather-camera-thesis\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq3IR95NMYor"
      },
      "outputs": [],
      "source": [
        "# Extraction Dataset - Time to execute: about 13 minutes\n",
        "PATH_TO_UNZIP_FILES = \"/home\"\n",
        "!unzip -o \"/content/drive/Shareddrives/WeatherCamera/Archives/Dataset.zip\" -d $PATH_TO_UNZIP_FILES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yshUWjnkMT3R"
      },
      "source": [
        "Dataset Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpUHHXdMJ4oU"
      },
      "outputs": [],
      "source": [
        "#If you already have the dataset annotations in Shared Drives\n",
        "!mkdir /home/weather-camera-thesis/data/\n",
        "!cp \"/content/drive/Shareddrives/WeatherCamera/dataset_annotation.csv\" \"/home/weather-camera-thesis/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KInhQGGM_Q3"
      },
      "outputs": [],
      "source": [
        "#If you don't have the dataset annotations in Shared Drives\n",
        "!python /home/weather-camera-thesis/src/generate_annotations.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6w6lDaWIvO3"
      },
      "source": [
        "Total Images from Dataset:  2.020.449\n",
        "\n",
        "* **dry**: 1.167.685 - **wet**: 845.771 - **snow**: 5.386 - **fog**: 1.607 \n",
        "\n",
        "Percentuali:\n",
        "* **DRY**:  0.5779\n",
        "* **WET**:  0.4186\n",
        "* **SNOW**:  0.0026\n",
        "* **FOG**:  0.0007\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seeCbvfzNY94"
      },
      "source": [
        "Check Dataset Annotations from .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FITH0YacLWLT"
      },
      "outputs": [],
      "source": [
        "## READ CSV ANNOTATIONS\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pathlib\n",
        "\n",
        "HOME_PATH = os.path.abspath('../weather-camera-thesis/')\n",
        "CSV_PATH = os.path.join(os.path.abspath(os.path.join(pathlib.Path.cwd(), 'data')), \"dataset_annotation.csv\")\n",
        "CSV_PATH_COLAB = os.path.abspath('/home/weather-camera-thesis/data/dataset_annotation.csv')\n",
        "\n",
        "#HOME_PATH = os.path.abspath('../weather-camera-thesis/')\n",
        "#pathlib.Path.cwd() current folder\n",
        "#pathlib.Path.cwd().parent current parent folder\n",
        "\n",
        "classes = [\"dry\", \"wet\", \"snow\", \"fog\"]\n",
        "counter = {c:0 for c in classes}\n",
        "len_data = 0\n",
        "\n",
        "\n",
        "with open(CSV_PATH_COLAB, 'r') as file:\n",
        "    reader = csv.reader(file, delimiter = ',')\n",
        "    for row in reader:\n",
        "        c = row[1]\n",
        "        counter[c] += 1\n",
        "        len_data += 1\n",
        "\n",
        "DRY_P = counter[\"dry\"]/len_data\n",
        "WET_P = counter[\"wet\"]/len_data\n",
        "SNOW_P = counter[\"snow\"]/len_data\n",
        "FOG_P= counter[\"fog\"]/len_data\n",
        "\n",
        "#limiti = dry, wet, snow, fog --> 0.30, 0.29, 0.12, 0.29\n",
        "\n",
        "print(\"Total Images: \", len_data)\n",
        "print(counter)\n",
        "print(\"Check Sum: \", counter[\"dry\"] + counter[\"wet\"] + counter[\"snow\"] + counter[\"fog\"], \"\\n\")\n",
        "print(\"Percentage calculation:\")\n",
        "print(\"DRY: \", round(DRY_P, 4)*100, \"%\")\n",
        "print(\"WET: \", round(WET_P, 4)*100, \"%\")\n",
        "print(\"SNOW: \", round(SNOW_P, 4)*100, \"%\")\n",
        "print(\"FOG: \", round(FOG_P, 4)*100, \"%\")\n",
        "print(\"Check Percentage: \", (DRY_P + WET_P + SNOW_P + FOG_P)*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8hgYcphQY3F"
      },
      "source": [
        "Partition Dataset from .csv Annotations and Moving Symbolic links-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhsNCVjEKyIO"
      },
      "outputs": [],
      "source": [
        "##PARTITION DATASET (ANNOTATIONS)\n",
        "#Time to execute to create new partition_info: about 2 hours and 30 minutes\n",
        "\n",
        "from copy import deepcopy\n",
        "import shutil\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "NEW_DATASET_PATH = os.path.abspath('/home/weather-camera-thesis/data/dataset')\n",
        "CSV_PATH_COLAB = os.path.abspath('/home/weather-camera-thesis/data/dataset_annotation.csv')\n",
        "CODE_PATH_COLAB = os.path.abspath('/content/drive/Shareddrives/WeatherCamera/Project Saves/Partition Info')\n",
        "\n",
        "def print_filenames(path:str, filenames:list)->None:\n",
        "    with open(path,\"w\") as f:\n",
        "        for item in filenames:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "def check_ds_distribution(counter:dict)->bool:\n",
        "    # check if the new created dataset respects the orginal dataset distribution #\n",
        "    #limits = dry, wet, snow, fog --> 0.30, 0.29, 0.12, 0.29\n",
        "\n",
        "    limits_imported = [DRY_P, WET_P, SNOW_P, FOG_P]\n",
        "    #limits = [0.30, 0.29, 0.12, 0.29]\n",
        "\n",
        "    tot = sum(list(counter.values()))\n",
        "    for i, k in enumerate(list(counter.keys())):\n",
        "        res = round(counter[k]/tot, 2)\n",
        "        if not (limits_imported[i]-0.02<=res<=limits_imported[i]+0.02):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def split(ratio, data, folder_info):\n",
        "    print(\"Annotations of .csv: \", len(data))\n",
        "    num_images = len(data)\n",
        "    num_test_images = math.ceil(ratio*num_images)\n",
        "    print(\"Train: {}, Test: {}\".format(num_images-num_test_images, num_test_images))\n",
        "\n",
        "    classes = [\"dry\", \"wet\", \"snow\", \"fog\"]\n",
        "    counter = {c:0 for c in classes}\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        print(\"TEST\")\n",
        "        images_copy = deepcopy(data)\n",
        "        for k in list(counter.keys()): counter[k]=0 # clear dict\n",
        "        filenames = []\n",
        "        for i in tqdm(range(num_test_images)):\n",
        "            idx = random.randint(0, len(images_copy)-1)\n",
        "            filename = images_copy[idx]\n",
        "            filenames.append(filename)\n",
        "            c = filename[1]\n",
        "            counter[c] +=1\n",
        "            images_copy.remove(filename)\n",
        "        done = check_ds_distribution(counter)\n",
        "        #print(\"TEST counter: \", counter)\n",
        "\n",
        "        # if distribution is respected, process remaining images-annotations for training set\n",
        "        if done:\n",
        "            print(\"TRAIN\")\n",
        "            for k in list(counter.keys()): counter[k]=0 # clear dict\n",
        "            for filename in tqdm(images_copy):\n",
        "                c = filename[1]\n",
        "                counter[c] +=1\n",
        "            # print(counter)\n",
        "            done = done and check_ds_distribution(counter)\n",
        "            #print(\"TRAIN counter: \", counter)\n",
        "    \n",
        "    print(\"Split done!\")\n",
        "    print_filenames(os.path.join(folder_info, \"test_filenames.txt\"), filenames)\n",
        "    print_filenames(os.path.join(folder_info, \"train_filenames.txt\"), images_copy)\n",
        "    return filenames, images_copy\n",
        "\n",
        "\n",
        "def adjust_read(folder, txt_file):\n",
        "    with open(os.path.join(folder,txt_file),\"r\") as f:\n",
        "        val_filenames = f.readlines()\n",
        "\n",
        "    val_filenames = [f.rstrip() for f in val_filenames]\n",
        "    new_filenames = []\n",
        "\n",
        "    for f in val_filenames:\n",
        "        path1, path2 = f.split(',')\n",
        "        filename = [path1[2:-1],path2[2:-2]]\n",
        "        new_filenames.append(filename)\n",
        "    return new_filenames\n",
        "\n",
        "\n",
        "def recover_filenames(folder:str)->list:\n",
        "    # recover annotations for training and validation from file\"\n",
        "    val_filenames = adjust_read(folder, \"test_filenames.txt\")\n",
        "    train_filenames = adjust_read(folder, \"train_filenames.txt\")\n",
        "\n",
        "    return val_filenames, train_filenames\n",
        "\n",
        "\n",
        "def move_images(val_filenames:list, train_filenames:list, dest:str):\n",
        "    # starting from lists of filename, move annotations in the right folders #\n",
        "    train_dir = os.path.join(dest, 'train')\n",
        "    test_dir = os.path.join(dest, 'test')\n",
        "\n",
        "    #Creation folders\n",
        "    if not os.path.exists(train_dir):\n",
        "        os.makedirs(train_dir)\n",
        "        os.makedirs(os.path.join(train_dir, \"dry\"))\n",
        "        os.makedirs(os.path.join(train_dir, \"fog\"))\n",
        "        os.makedirs(os.path.join(train_dir, \"wet\"))\n",
        "        os.makedirs(os.path.join(train_dir, \"snow\"))\n",
        "\n",
        "\n",
        "    if not os.path.exists(test_dir):\n",
        "        os.makedirs(test_dir)\n",
        "        os.makedirs(os.path.join(test_dir, \"dry\"))\n",
        "        os.makedirs(os.path.join(test_dir, \"fog\"))\n",
        "        os.makedirs(os.path.join(test_dir, \"wet\"))\n",
        "        os.makedirs(os.path.join(test_dir, \"snow\"))\n",
        "\n",
        "    print(\"TEST\")\n",
        "    for f in tqdm(val_filenames):\n",
        "        #shutil.copy(f[0], os.path.join(test_dir, f[1]))\n",
        "        img = f[0].rsplit('/', 1)\n",
        "        os.symlink(f[0], test_dir+\"/\"+f[1]+\"/\"+str(img[-1]))\n",
        "    \n",
        "    print(\"TRAIN\")\n",
        "    for f in tqdm(train_filenames):\n",
        "        #shutil.copy(f[0], os.path.join(train_dir, f[1]))\n",
        "        img = f[0].rsplit('/', 1)\n",
        "        os.symlink(f[0], train_dir+\"/\"+f[1]+\"/\"+str(img[-1]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    with open(CSV_PATH_COLAB, newline='') as f:\n",
        "        reader = csv.reader(f)\n",
        "        data = list(reader)\n",
        "        \n",
        "    folder = os.path.join(CODE_PATH_COLAB, \"partition_info1\") ######\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        ratio = 0.1\n",
        "        print(\"Create new Partition Dataset with split ratio: {}\".format(ratio))\n",
        "        val_filenames, train_filenames = split(ratio, data, folder)\n",
        "\n",
        "    else:\n",
        "        print(\"Found an existing Partition Info: \", folder)\n",
        "        val_filenames, train_filenames = recover_filenames(folder)\n",
        "        print(\"Split recovered !\")\n",
        "\n",
        "    print(\"Moving symbolic links-image...\")\n",
        "    move_images(val_filenames, train_filenames, NEW_DATASET_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxZGFR9dT2KM"
      },
      "source": [
        "Build Model: MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFaZyi7_eBLe"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'    # Suppress TensorFlow logging (1)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "INPUT_SHAPE = (224,224,3)\n",
        "IMG_SIZE = 224\n",
        "N_CLASSES = 4 #dry, wet, snow, fog\n",
        "N_LAYERS_TO_TRAIN = 10 #to modify\n",
        "\n",
        "img_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomRotation(factor=0.15),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        layers.RandomFlip(mode=\"horizontal\"),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "    ],\n",
        "    name=\"img_augmentation\",\n",
        ")\n",
        "\n",
        "def build_model(freeze:bool=None)->tf.keras.Model:\n",
        "\n",
        "    #inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)) #\n",
        "    #x = img_augmentation(inputs)                         #\n",
        "    base_model = MobileNetV2(INPUT_SHAPE, include_top=False, weights='imagenet') #input_tensor=x # 154 layers\n",
        "    if freeze is not None:\n",
        "        if freeze: base_model.trainable = False\n",
        "        else:\n",
        "            for layer in base_model.layers:\n",
        "                if not isinstance(layer, layers.BatchNormalization):\n",
        "                    layer.trainable = True\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    #x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x) #DA RIMUOVERE INIZIALMENTE\n",
        "    model = layers.Dense(N_CLASSES, 'softmax')(x)\n",
        "    final_model = tf.keras.Model(inputs=base_model.input, outputs=model)\n",
        "    #print(final_model.summary())\n",
        "    return final_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJxY2u37UPpf"
      },
      "source": [
        "Setting of the Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2UdOuURd1-c"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "NEW_DATASET_PATH = os.path.abspath('/home/weather-camera-thesis/data/dataset/train')\n",
        "\n",
        "n_experiment = \"?_experiment\" #specify the experiment to execute\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/checkpoint/\".format(n_experiment)\n",
        "LOGS_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/logs/\".format(n_experiment)\n",
        "TB_LOGS_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/tb_logs/\".format(n_experiment)\n",
        "DOC_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/doc\".format(n_experiment)\n",
        "\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_PATH): os.makedirs(CHECKPOINT_PATH)\n",
        "if not os.path.exists(LOGS_PATH): os.makedirs(LOGS_PATH)\n",
        "if not os.path.exists(TB_LOGS_PATH): os.makedirs(TB_LOGS_PATH)\n",
        "if not os.path.exists(DOC_PATH): os.makedirs(DOC_PATH)    \n",
        "\n",
        "def train():\n",
        "\n",
        "    TRAIN_DIM = 1454724 #\n",
        "    VAL_DIM = 363680 #\n",
        "    print(\"TRAIN Dim: \", TRAIN_DIM, \", VAL Dim: \", VAL_DIM)\n",
        "    CLASSIFICATOR_INPUT_SIZE = (224,224)\n",
        "\n",
        "    batch_size = 32\n",
        "    epochs = 15\n",
        "    patience= 10\n",
        "    learning_rate = 0.00003\n",
        "\n",
        "    freeze_model = False #to check\n",
        "    resume_training = False #to check\n",
        "    model_path = os.path.abspath(\"/\") #to check\n",
        "\n",
        "    # model\n",
        "    model = build_model(freeze_model)\n",
        "    if resume_training:\n",
        "        model.load_weights(model_path)\n",
        "    \n",
        "    #lr_schedule = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, int(args.epochs*(TRAIN_DIM/args.batch_size)))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "                loss=tf.keras.losses.categorical_crossentropy, \n",
        "                metrics=[tf.keras.metrics.categorical_accuracy]\n",
        "                )\n",
        "\n",
        "    train_datagen = ImageDataGenerator(validation_split=0.20)\n",
        "    #test_datagen = ImageDataGenerator()\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=NEW_DATASET_PATH,\n",
        "        target_size=CLASSIFICATOR_INPUT_SIZE,\n",
        "        #color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",\n",
        "        subset='training',\n",
        "        shuffle=True,\n",
        "        seed=556\n",
        "    )\n",
        "\n",
        "    valid_generator = train_datagen.flow_from_directory(\n",
        "        directory=NEW_DATASET_PATH,\n",
        "        target_size=CLASSIFICATOR_INPUT_SIZE,\n",
        "        #color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",\n",
        "        subset='validation',\n",
        "        shuffle=True,\n",
        "        seed=556\n",
        "    )\n",
        "\n",
        "    class_weights = dict(zip(np.unique(valid_generator.classes), class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(valid_generator.classes), y=valid_generator.classes))) \n",
        "    print(\"Class Weights: \", class_weights)\n",
        "\n",
        "    #resize images\n",
        "    #train_ds = train_ds.map(lambda image, label: (tf.image.resize(image, CLASSIFICATOR_INPUT_SIZE), label))\n",
        "    #val_ds = val_ds.map(lambda image, label: (tf.image.resize(image, CLASSIFICATOR_INPUT_SIZE), label))\n",
        "    #train_ds = train_ds.prefetch(buffer_size=32)\n",
        "    #val_ds = val_ds.prefetch(buffer_size=32)\n",
        "\n",
        "    # callbacks --> val_loss, prima c'era \"val_categorical_accuracy\"\n",
        "    filepath = 'model-epoch_{epoch:02d}.hdf5'\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(os.path.join(CHECKPOINT_PATH, filepath), save_weights_only=True, verbose=1, save_best_only=True)\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=patience,verbose=1,monitor='val_loss',mode='auto')\n",
        "    history_logger_cb = tf.keras.callbacks.CSVLogger((os.path.join(LOGS_PATH, 'training_log.csv')), separator=\",\", append=True)\n",
        "    tensorboad_cb = tf.keras.callbacks.TensorBoard(log_dir=TB_LOGS_PATH, write_graph=False)\n",
        "    callbacks = [checkpoint_cb,early_stopping_cb,history_logger_cb,tensorboad_cb]\n",
        "\n",
        "    hist = model.fit(train_generator, validation_data = valid_generator, epochs=epochs, verbose = 1, callbacks=callbacks, shuffle = False, class_weight=class_weights,\n",
        "                steps_per_epoch=int(np.ceil(TRAIN_DIM / batch_size)),validation_steps=int(np.ceil(VAL_DIM / batch_size)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IUOHGKmeu8P"
      },
      "outputs": [],
      "source": [
        "## Execute the Training\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gssrTqPUurt"
      },
      "source": [
        "Plotting history of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNk2lOsZTpET"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N_CLASSES = 4\n",
        "image_size = (224, 224)\n",
        "\n",
        "def plot_hist(csv_path:str, save:bool, imgs_path:str=None)->list:\n",
        "    with open(csv_path, mode='r') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        hist ={\"epoch\":[], \"categorical_accuracy\":[], \"val_categorical_accuracy\":[],\"loss\":[],\"val_loss\":[]}\n",
        "        for row in csv_reader:\n",
        "            hist[\"epoch\"].append(float(row[\"epoch\"]))\n",
        "            hist[\"categorical_accuracy\"].append(float(row[\"categorical_accuracy\"]))\n",
        "            hist[\"val_categorical_accuracy\"].append(float(row[\"val_categorical_accuracy\"]))\n",
        "            hist[\"loss\"].append(float(row[\"loss\"]))\n",
        "            hist[\"val_loss\"].append(float(row[\"val_loss\"]))\n",
        "\n",
        "        n_epochs = len(hist[\"epoch\"])\n",
        "\n",
        "        plt.figure() \n",
        "        plt.title(\"LOSS\")\n",
        "        plt.plot(hist[\"loss\"], color='blue', label='loss')\n",
        "        plt.plot(hist[\"val_loss\"], color='orange', label='Val_Loss')\n",
        "        plt.xticks(range(0,n_epochs))\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.legend([\"train\", \"validation\"], loc=\"best\")\n",
        "\n",
        "        if save: \n",
        "            path_loss_fig = imgs_path+\"/loss_{}.png\".format(n_epochs)\n",
        "            plt.savefig(path_loss_fig)\n",
        "        else: plt.show()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"ACCURACY\")\n",
        "        plt.plot(hist[\"categorical_accuracy\"], color='blue', label='Accuracy')\n",
        "        plt.plot(hist[\"val_categorical_accuracy\"], color='orange',label='Val_accuracy')\n",
        "        plt.xticks(range(0,n_epochs))\n",
        "        plt.ylabel(\"accuracy\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.legend([\"train\", \"validation\"], loc=\"best\")\n",
        "        \n",
        "        if save: \n",
        "            path_acc_fig = imgs_path+\"/acc_{}.png\".format(n_epochs)\n",
        "            plt.savefig(path_acc_fig)\n",
        "        else: plt.show()\n",
        "\n",
        "        if save: return [path_loss_fig, path_acc_fig]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0PeCGSxRFBs"
      },
      "outputs": [],
      "source": [
        "## Plot\n",
        "n_experiment = \"?_experiment\" #specify the experiment to visualize\n",
        "\n",
        "TRAIN_LOG_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/logs/training_log.csv\".format(n_experiment)\n",
        "DOC_PATH = \"/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/doc\".format(n_experiment)\n",
        "\n",
        "\n",
        "plot_hist(TRAIN_LOG_PATH, save=True, imgs_path=DOC_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ZDhGxiG5U3"
      },
      "source": [
        "Testing of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q5Rm1mOCG8fV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'    # Suppress TensorFlow logging (1)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score, balanced_accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "\n",
        "CLASSIFICATOR_INPUT_SIZE = (224,224)\n",
        "\n",
        "n_experiment = \"?_experiment\" #specify the experiment to test\n",
        "n_partition_info = \"partition_info?\" #specify the partition_info to test\n",
        "\n",
        "model_path = \"/\" #to check\n",
        "TEST_SET_PATH = os.path.abspath('/content/drive/Shareddrives/WeatherCamera/Project Saves/Partition Info/{}/test_filenames.txt'.format(n_partition_info))\n",
        "OUTPUT_PATH = os.path.abspath('/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/doc/results.txt'.format(n_experiment))\n",
        "\n",
        "category_index_classifier = {\"dry\": 0, \"fog\": 1, \"snow\": 2, \"wet\": 3}\n",
        "\n",
        "\"\"\"\n",
        "## Run inference on new data\n",
        "\"\"\"\n",
        "\n",
        "def inference_data(model_path):\n",
        "    \n",
        "    print(\"Loading model...\")\n",
        "    model = build_model()\n",
        "    model.load_weights(model_path)\n",
        "    print(\"Loading model...DONE\")\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    with open(TEST_SET_PATH) as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [f.rstrip() for f in lines]\n",
        "\n",
        "    print(\"Inference data:\")\n",
        "    for f in tqdm(lines):\n",
        "        path1, path2 = f.split(',')\n",
        "        filename = [path1[2:-1],path2[2:-2]]\n",
        "\n",
        "        img_path = filename[0]\n",
        "\n",
        "        img = keras.preprocessing.image.load_img(img_path, target_size=CLASSIFICATOR_INPUT_SIZE)\n",
        "        img_array = keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "\n",
        "        true_label = category_index_classifier[filename[1]]\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        predictions = model.predict(img_array)\n",
        "        score = predictions[0]\n",
        "        pred_label = np.argmax(score)\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "\n",
        "    labels = list(category_index_classifier.keys())\n",
        "\n",
        "    print(\"Building confusion matrix...\")\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
        "    disp.plot()\n",
        "    plt.savefig(os.path.abspath('/content/drive/Shareddrives/WeatherCamera/Project Saves/{}/doc/cm_test.png'.format(n_experiment)))\n",
        "    matrix = pd.DataFrame(matrix, index=[\"true:{}\".format(x) for x in labels], columns=[\"pred:{}\".format(x) for x in labels])\n",
        "    print(\"Building confusion matrix...DONE\")\n",
        "    print(\"Confusion Matrix:\\n\", matrix)\n",
        "    \n",
        "    print(\"Computing Metrics...\")\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "    rec_score = recall_score(y_true, y_pred, average='weighted')\n",
        "    prec_score = precision_score(y_true, y_pred, average='weighted')\n",
        "    f1score = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(\"Computing Metrics...DONE\")\n",
        "    print(\"Accuracy Score: \", round(acc, 4))\n",
        "    print(\"Balanced Accuracy Score: \", round(bal_accuracy, 4))\n",
        "    print(\"Recall Score: \", round(rec_score, 4))\n",
        "    print(\"Precision Score: \", round(prec_score, 4))\n",
        "    print(\"F1-Score: \", round(f1score, 4))\n",
        "\n",
        "    with open(OUTPUT_PATH,\"w\") as f:\n",
        "        f.write(\"CONFUSION MATRIX\\n\"+matrix.to_string())\n",
        "        f.write(\"\\n\\nACCURACY:\\n\"+str(round(acc, 4)))\n",
        "        f.write(\"\\n\\nBALANCED ACCURACY:\\n\"+str(round(bal_accuracy, 4)))\n",
        "        f.write(\"\\n\\nPRECISION:\\n\"+str(round(prec_score, 4)))\n",
        "        f.write(\"\\n\\nRECALL:\\n\"+str(round(rec_score, 4)))\n",
        "        f.write(\"\\n\\nF1-SCORE:\\n\"+str(round(f1score, 4)))\n",
        "\n",
        "\n",
        "inference_data(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}